{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup \n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from pykospacing import spacing\n",
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()\n",
    "sentiment = pd.read_csv('polarity.csv')\n",
    "word_list = sentiment['ngram'].tolist()\n",
    "label = sentiment['max.value'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "Category_average_img_count = {'IT·컴퓨터': 10.128282828282828,\n",
    " '건강·의학': 8.582714382174206,\n",
    " '공연·전시': 12.55934065934066,\n",
    " '교육·학문': 10.158496732026144,\n",
    " '국내여행': 26.041441441441442,\n",
    " '드라마·방송': 11.486739469578783,\n",
    " '등산·낚시·레저': 23.269372693726936,\n",
    " '만화·애니': 17.366032210834554,\n",
    " '맛집': 18.8370720188902,\n",
    " '사진': 13.06946876824285,\n",
    " '스포츠': 8.87403314917127,\n",
    " '시사·인문·경제': 5.872727272727273,\n",
    " '어학·외국어': 11.586510263929618,\n",
    " '와인·술': 7.527027027027027,\n",
    " '육아·결혼': 15.317307692307692,\n",
    " '자동차': 17.298047276464544,\n",
    " '차·커피·디저트': 14.595382746051033,\n",
    " '패션·뷰티': 14.654545454545454}\n",
    "\n",
    "Category_average_text_len = {'IT·컴퓨터': 1245.679798090909,\n",
    " '건강·의학': 1300.688048757597,\n",
    " '공연·전시': 1468.787912648351,\n",
    " '교육·학문': 1380.7295753839874,\n",
    " '국내여행': 1626.1315322342334,\n",
    " '드라마·방송': 1351.5803434321374,\n",
    " '등산·낚시·레저': 1542.4686354612543,\n",
    " '만화·애니': 1016.5578340849175,\n",
    " '맛집': 1341.5159389492333,\n",
    " '사진': 764.9451266549917,\n",
    " '스포츠': 1743.537016895028,\n",
    " '시사·인문·경제': 1863.716667424242,\n",
    " '어학·외국어': 12064.428153108505,\n",
    " '와인·술': 1041.2500004054054,\n",
    " '육아·결혼': 1268.576924326924,\n",
    " '자동차': 2040.659815323742,\n",
    " '차·커피·디저트': 1292.0935606075336,\n",
    " '패션·뷰티': 1270.7898993686888}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Spacing_text(text_list):\n",
    "    spacing_list = []\n",
    "    for i in text_list:\n",
    "        if len(i) < 197:\n",
    "            spacing_list.append(spacing(i))\n",
    "        else:\n",
    "            iteration = int(len(i) / 197)\n",
    "            mod = len(i) % 197\n",
    "            start = 0\n",
    "            end = 197\n",
    "            check = 0\n",
    "            while True:\n",
    "                # 시행횟수 < 몫\n",
    "                if check < iteration:\n",
    "                    spacing_list.append(spacing(i[start:end]))\n",
    "                    start += 197\n",
    "                    end += 197\n",
    "                    check += 1\n",
    "                else:\n",
    "                    # 마지막 횟수 + 나머지 더 slice\n",
    "                    spacing_list.append(spacing(i[iteration * 197:(iteration * 197) + mod]))\n",
    "                    break\n",
    "    return spacing_list\n",
    "\n",
    "def remove_odd(x):\n",
    "    x = re.sub(\"nbsp\", \" \", x)\n",
    "    x = re.sub(\"\\xa0\", \"\", x)\n",
    "    x = re.sub(\"\\u200b\", \"\", x)\n",
    "    x = re.sub(\"\\n\", \"\", x)\n",
    "    x = re.sub(\"\\t\", \"\", x)\n",
    "    x = re.sub('   ', ' ', x)\n",
    "    return x\n",
    "\n",
    "def tfidf_vectorizer(Text):\n",
    "    try:\n",
    "        return v_load.transform([Text]).toarray().flatten()\n",
    "    except:\n",
    "        return ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class textclass:\n",
    "    def Extract_structure_and_tag(User_id, Post_id):\n",
    "        url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "        r = requests.get(url)\n",
    "        bs = BeautifulSoup(re.sub('&nbsp;', ' ', r.text).encode(\"utf-8\"), \"html.parser\")\n",
    "        # structure\n",
    "        structure = bs.find(\"div\", {\"id\": \"postViewArea\"})\n",
    "        if structure == None:\n",
    "            structure = bs.find(\"div\", {\"class\", \"se_component_wrap sect_dsc __se_component_area\"})\n",
    "\n",
    "        structure_p_img_tag = structure.find_all(['p', 'img'])\n",
    "        structure_dict = {'structure': structure, 'structure_p_img_tag': structure_p_img_tag}\n",
    "        # structure_p_img_tag : p,img tag만 extract\n",
    "        # structure : 모든 tag 가져오기\n",
    "        return structure_dict\n",
    "\n",
    "    # Extract_structure_and_tag 함수의 'structure_p_img_tag' 값을 가져와야함.\n",
    "\n",
    "    def HTML_preprocessing(structure_p_img_tag):\n",
    "        # only tag & text extract\n",
    "        tag_list = []\n",
    "        text_list = []\n",
    "        for i in structure_p_img_tag:\n",
    "            # p_tag만 불러오기\n",
    "            if \"<p\" in (str(i)):\n",
    "                tag_list.append('<p>')\n",
    "                # img만 있을 때\n",
    "\n",
    "                if '<img' in str(i):\n",
    "                    for j in i:\n",
    "                        try:\n",
    "                            if len(j.text) > 1:\n",
    "                                tag_list.append('<br>')\n",
    "                                text_list.append(j.text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # img가 아닌 경우 span tag가 더 있을 때\n",
    "                elif '<span' in str(i):\n",
    "                    for j in i:\n",
    "                        if '<br' in str(j):\n",
    "                            text_list.append(j.text)\n",
    "                            # br_tag가 2개 이상 있을 때\n",
    "\n",
    "                            if len(j.findAll('br')) > 2:\n",
    "                                for _ in range(0, len(j.findAll('br'))):\n",
    "                                    tag_list.append('<br>')\n",
    "\n",
    "                            # br_tag가 1개 있을 때\n",
    "                            else:\n",
    "                                tag_list.append('<br>')\n",
    "\n",
    "                        # span은 있지만 br tag가 없을 때\n",
    "                        else:\n",
    "                            try:\n",
    "                                text_list.append(j.text)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                # 그냥 p_tag만 있을 때 br_tag 추가\n",
    "                else:\n",
    "                    # 글이 있을 때\n",
    "                    if len(i.text) > 1:\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                    # 글 없이 br tag만 있을 때\n",
    "                    else:\n",
    "                        tag_list.append('<br>')\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                # P_tag 끝맽음\n",
    "                tag_list.append('</p>')\n",
    "\n",
    "            else:\n",
    "                tag_list.append('<img>')\n",
    "\n",
    "        text_list = list(map(remove_odd, text_list))\n",
    "        filter_text = list(filter(lambda x: len(x) > 1, text_list))\n",
    "\n",
    "        Text = \" \".join(list(filter(lambda x: len(x) > 1, map(lambda x: x.strip(), text_list))))\n",
    "        Text = re.sub('\\n', '', Text)\n",
    "        Text = re.sub('\\t', '', Text)\n",
    "        Space_text = \" \".join(Spacing_text(filter_text))\n",
    "        Count_space_mistake = len(Space_text) - len(Text)\n",
    "\n",
    "        # only tag\n",
    "        Structure_only_tag = \"|\".join(tag_list)\n",
    "        Structure_only_tag_df = pd.DataFrame({'text': [Structure_only_tag]})\n",
    "        array_temp = Structure_only_tag_df['text'].apply(\n",
    "            lambda x: \" img \".join(list(map(lambda x: 'text' if len(x) > 3 else '', x.split('<img>')))).strip().replace(\n",
    "                '  ', ' ')).values\n",
    "        refined_structure = ''.join(array_temp)\n",
    "\n",
    "        HTML_preprocessing = {'Text': Text, 'refined_structure': refined_structure,\n",
    "                              'Count_space_mistake': Count_space_mistake}\n",
    "\n",
    "        return HTML_preprocessing\n",
    "\n",
    "    def sentimental_analysis(text):\n",
    "        pos_word_list = []\n",
    "        neg_word_list = []\n",
    "        \n",
    "        pos_ratio = 0.000000001\n",
    "        neg_ratio = 0.000000001\n",
    "        subjectivity = 0.000000001\n",
    "        polarity = 0.000000001\n",
    "        senti_diffs_per_ref = 0.000000001\n",
    "\n",
    "        if text == '':\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list\n",
    "        else:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            text = text.split(' ')\n",
    "            n = len(text)\n",
    "            for i in text:\n",
    "                i = remove_odd(i)\n",
    "                pre = kkma.pos(i)\n",
    "                test = ';'.join(['/'.join(i) for i in pre])\n",
    "                if test in word_list:\n",
    "                    if label[word_list.index(test)] == 'POS':\n",
    "                        pos += 1\n",
    "                        pos_word_list.append(test)\n",
    "                    elif label[word_list.index(test)] == 'NEG':\n",
    "                        neg += 1\n",
    "                        neg_word_list.append(test)\n",
    "            try:\n",
    "                pos_ratio = pos / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                neg_ratio = neg / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subjectivity = (neg + pos) / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                polarity = (neg - pos) / (neg + pos)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                senti_diffs_per_ref = (pos - neg) / n\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list\n",
    "\n",
    "    def check_First_second(Text):\n",
    "        first_person = ['나/NP', '저/NP', '내/NP', '제/NP', '저희/NP', '우리/NP']\n",
    "        second_person = ['너/NP', '자네/NP', '당신/NP', '그대/NP', '그쪽/NP', '너희/NP', '자기/NP']\n",
    "        First = 0\n",
    "        Second = 0\n",
    "        if Text == '':\n",
    "            check_First_second_dict = {'First': First, 'Second': Second}\n",
    "            return check_First_second_dict\n",
    "        else:\n",
    "            text = kkma.pos(Text)\n",
    "            for i in text:\n",
    "                temp = \"/\".join(i)\n",
    "                if temp in first_person:\n",
    "                    First += len(temp.split('/')[0])\n",
    "                if temp in second_person:\n",
    "                    Second += len(temp.split('/')[0])\n",
    "            check_First_second_dict = {'First_ratio': First/len(Text), 'Second_ratio': Second/len(Text)}\n",
    "            return check_First_second_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class otherclass:\n",
    "    def effort_check(Category,Text_len,Img_count):\n",
    "        effort_text = Text_len / Category_average_text_len[Category]\n",
    "        effort_img = Img_count / Category_average_img_count[Category]\n",
    "\n",
    "        effort_dict = {'effort_ratio':effort_text,'effort_img_ratio':effort_img}\n",
    "        return effort_dict\n",
    "\n",
    "    def Tag_count(url):\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        driver.get(url)\n",
    "        driver.implicitly_wait(10)\n",
    "        tag_count = len(re.sub('\\n','',driver.find_element_by_class_name('wrap_tag').text).strip().split('#')[1:])\n",
    "        tag_dict = {'tag_count':tag_count}\n",
    "        driver.close()\n",
    "        return tag_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class htmlclass:\n",
    "    def Extract_Alignment(structure):\n",
    "        # Align\n",
    "        split_structure =  remove_odd(str(structure)).split('>')\n",
    "        center= 0\n",
    "        left = 0\n",
    "        right = 0\n",
    "        justify = 0\n",
    "        for item in split_structure:\n",
    "            if 'align' or 'ALIGN' in str(item):\n",
    "                if 'center' in str(item):\n",
    "                    center +=1\n",
    "                if 'left' in str(item):\n",
    "                    left += 1\n",
    "                if 'right' in str(item):\n",
    "                    right +=1\n",
    "                if 'justify' in str(item):\n",
    "                    justify +=1\n",
    "        Align = [left, center, right, justify]\n",
    "        return Align\n",
    "\n",
    "    def Extract_Sticker_count(structure):\n",
    "        Sticker_count = 0\n",
    "        # Sticker_count로 수정\n",
    "        sticker_img = structure.find_all('a')\n",
    "        for i in sticker_img:\n",
    "            if 'sticker' in str(i):\n",
    "                Sticker_count += 1\n",
    "        return Sticker_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class chorme_class:\n",
    "    def user_information(mobile_url,opening_url):\n",
    "        driver = webdriver.Chrome('./chromedriver')\n",
    "        \n",
    "        driver.get(mobile_url)\n",
    "        driver.implicitly_wait(3)\n",
    "        \n",
    "        # 존재하지 않는 게시물 클릭\n",
    "        driver.find_element_by_class_name(\"btn_area\").click()\n",
    "        driver.implicitly_wait(3)\n",
    "        \n",
    "        # Blog_name, Blog_nickname, Blog_mobile_profile_img, Blog_info_text\n",
    "        Blog_name = driver.find_element_by_css_selector('#rego_cover > div.cover_cont > div.tit_area > h2 > a > span').text\n",
    "        Blog_nickname =driver.find_element_by_class_name(\"user_name\").text\n",
    "\n",
    "        # Count_neighbors\n",
    "        neighbors_string = re.sub(\",\",\"\",driver.find_element_by_class_name(\"count_buddy\").text)\n",
    "        Count_neighbors = int(re.findall('\\d+', neighbors_string)[0])\n",
    "\n",
    "        # Count_visitors\n",
    "        visitor_stirng = driver.find_elements_by_class_name('count')[0].text\n",
    "        Count_visitors = re.sub(\",\",\"\",visitor_stirng.split(\"전체\")[1]).strip()\n",
    "        # Opening URL\n",
    "        driver.get(opening_url)\n",
    "        driver.implicitly_wait(3)\n",
    "        driver.find_element_by_id('category2').click()\n",
    "        years = driver.find_elements_by_css_selector('#post-area > div:nth-child(4) > table:nth-child(2) > tbody > tr > td > table > tbody > tr > td > table > tbody > tr:nth-child(3) > td:nth-child(2)')\n",
    "        for year in years:\n",
    "            if len(year.text) > 4:\n",
    "                blog_opening_date = year.text\n",
    "        driver.close()\n",
    "        \n",
    "        user_information_dict = {'Blog_name':Blog_name, 'Blog_nickname':Blog_nickname, 'Count_neighbors':Count_neighbors, 'Count_visitors':Count_visitors, 'blog_opening_date':blog_opening_date}\n",
    "        \n",
    "        return user_information_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "User_id = 'newpark314'\n",
    "Post_id = '221387605004'\n",
    "Category = '맛집'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "mobile_url = \"http://m.blog.naver.com/PostView.nhn?blogId=\"+ User_id\n",
    "opening_url = 'http://blog.naver.com/profile/intro.nhn?blogId='+ User_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text_len,Question_count,Sentiment(pos_ratio,neg_ratio,subjectivity,polarity,sentiment_diff_ref)\n",
    "# Count_Space_mistake, First_ratio, Second_ratio\n",
    "\n",
    "structure = textclass.Extract_structure_and_tag(User_id,Post_id)\n",
    "all_tag = structure['structure']\n",
    "p_img_tag = structure['structure_p_img_tag']\n",
    "HTML_preprocessing = textclass.HTML_preprocessing(p_img_tag)\n",
    "text = HTML_preprocessing['Text']\n",
    "\n",
    "\n",
    "# variables\n",
    "Text_len = len(text)\n",
    "Count_Space_mistake = HTML_preprocessing['Count_space_mistake']\n",
    "Question_count = text.count('?')\n",
    "sentiment_pre = textclass.sentimental_analysis(text)\n",
    "sentiment = sentiment_pre[0]\n",
    "pos_word = sentiment_pre[1]\n",
    "neg_word = sentiment_pre[2]\n",
    "first_second = textclass.check_First_second(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alignment(Left,Center,Right,Justify), Sticker_count\n",
    "# variables\n",
    "Alignment = htmlclass.Extract_Alignment(all_tag)\n",
    "Sticker_count = htmlclass.Extract_Sticker_count(all_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effort(effort_ratio,effort_img_ratio), Tag_count\n",
    "refined_structure = HTML_preprocessing['refined_structure']\n",
    "\n",
    "# variables\n",
    "Img_count = refined_structure.count('img')\n",
    "Effort = otherclass.effort_check(Category,Text_len,Img_count)\n",
    "Tag_count = otherclass.Tag_count(url)['tag_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 사용자 정보"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용자 정보\n",
    "user_information = chorme_class.user_information(mobile_url,opening_url)\n",
    "\n",
    "## user variables\n",
    "Blog_name = user_information['Blog_name']\n",
    "Blog_nickname = user_information['Blog_nickname']\n",
    "Count_neighbors = user_information['Count_neighbors']\n",
    "Count_visitors = user_information['Count_visitors']\n",
    "blog_opening_date = user_information['blog_opening_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf, keras model, cluster model\n",
    "#Load it later\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "transformer = TfidfTransformer()\n",
    "loaded_vec = TfidfVectorizer(decode_error=\"replace\",vocabulary=pickle.load(open(\"[Structure_tag]TFIDF_features_100.pkl\", \"rb\")))\n",
    "transformer = TfidfTransformer()\n",
    "v_load = pickle.load(open(\"[Structure_tag]TFIDF_features_100.pkl\", \"rb\"))\n",
    "\n",
    "# variable\n",
    "Structure_13 = tfidf_vectorizer(refined_structure)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_1 = [Question_count,first_second['First_ratio'],first_second['Second_ratio'],Tag_count,\n",
    "sentiment['pos_ratio'],sentiment['neg_ratio'],sentiment['subjectivity'],sentiment['polarity'],sentiment['senti_diffs_per_ref'],\n",
    "Sticker_count,Text_len,Count_Space_mistake,Effort['effort_ratio'],Effort['effort_img_ratio'],\n",
    "Alignment[0],Alignment[1],Alignment[2],Alignment[3]]\n",
    "list_2 = Structure_13.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_dataset = list_1 + list_2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "origin_df = pd.DataFrame(total_dataset).T\n",
    "origin_df.columns = ['Question_count','First_ratio','Second_ratio','Tag_count','pos_ratio',\n",
    " 'neg_ratio','subjectivity','polarity','senti_diffs_per_ref','Sticker_count','Text_len','Count_space_mistake',\n",
    "   'effort_ratio','effort_img_ratio','Left','Center','Right','Justify',\n",
    "   'img img img img img','img img img img text','img img img text img','img img text img img','img img text img text',\n",
    " 'img text img img img','img text img img text','img text img text img','text img img img img','text img img img text',\n",
    " 'text img img text img','text img text img img','text img text img text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total_df = pd.read_csv('[Final]Total_csv.csv',sep='\\t')\n",
    "# Total_df = Total_df[['Question_count','First_ratio','Second_ratio','Tag_count','pos_ratio',\n",
    "#  'neg_ratio','subjectivity','polarity','senti_diffs_per_ref','Sticker_count','Text_len','Count_space_mistake',\n",
    "#    'effort_ratio','effort_img_ratio','Left','Center','Right','Justify',\n",
    "#  'img img img img img','img img img img text','img img img text img','img img text img img','img img text img text',\n",
    "#  'img text img img img','img text img img text','img text img text img','text img img img img','text img img img text',\n",
    "#  'text img img text img','text img text img img','text img text img text']]\n",
    "# x = Total_df.dropna()\n",
    "# col_name = x.columns.tolist()\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# scaler = StandardScaler()\n",
    "# scaler.fit(x)\n",
    "# scaled_x = scaler.transform(x)\n",
    "# scaled_x_df = pd.DataFrame(scaled_x)\n",
    "# scaled_x_df.columns = col_name\n",
    "# scalerfile = 'scaler.pkl'\n",
    "# pickle.dump(scaler, open(scalerfile, 'wb'))\n",
    "# scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "# test_scaled_set = scaler.transform(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Standard scaler model load\n",
    "import pickle\n",
    "scalerfile = 'scaler.pkl'\n",
    "scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "# transform data\n",
    "origin_df = origin_df.fillna(0)\n",
    "X_scaled = pd.DataFrame(scaler.transform(origin_df),columns = origin_df.columns)\n",
    "\n",
    "# keras model load\n",
    "from keras.models import load_model\n",
    "model = load_model('keras_model.h5')\n",
    "\n",
    "# data type reshape & predict probability\n",
    "prob = model.predict(X_scaled.values.reshape((1, 31))).item()\n",
    "# predict classes\n",
    "predict_classes = model.predict_classes(X_scaled.values.reshape((1, 31))).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster는 cluster model자체가 scaler로 된 모델이라 그냥 origin 값 집어 넣어야함.\n",
    "clusterfile = '8-means(0,1,2,5,6,7).pkl'\n",
    "cluster = pickle.load(open(clusterfile, 'rb'))\n",
    "# predict cluster class\n",
    "predict_cluster_class = cluster.predict(origin_df).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final save csv_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Predict_df = pd.DataFrame({'prob':prob,'predict_classes':predict_classes,'predict_cluster_class':predict_cluster_class},index=[0])\n",
    "user_df = pd.DataFrame({'Blog_name':Blog_name,'Blog_nickname':Blog_nickname,'Count_neighbors':Count_neighbors,'Count_visitors':Count_visitors,'blog_opening_date':blog_opening_date},index=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = pd.concat([X_scaled,Predict_df],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "value.to_csv('test_csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Question_count</th>\n",
       "      <th>First_ratio</th>\n",
       "      <th>Second_ratio</th>\n",
       "      <th>Tag_count</th>\n",
       "      <th>pos_ratio</th>\n",
       "      <th>neg_ratio</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>polarity</th>\n",
       "      <th>senti_diffs_per_ref</th>\n",
       "      <th>Sticker_count</th>\n",
       "      <th>...</th>\n",
       "      <th>img text img img text</th>\n",
       "      <th>img text img text img</th>\n",
       "      <th>text img img img img</th>\n",
       "      <th>text img img img text</th>\n",
       "      <th>text img img text img</th>\n",
       "      <th>text img text img img</th>\n",
       "      <th>text img text img text</th>\n",
       "      <th>prob</th>\n",
       "      <th>predict_classes</th>\n",
       "      <th>predict_cluster_class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.40729</td>\n",
       "      <td>1.32493</td>\n",
       "      <td>-0.152399</td>\n",
       "      <td>0.665565</td>\n",
       "      <td>0.57509</td>\n",
       "      <td>0.289281</td>\n",
       "      <td>0.551111</td>\n",
       "      <td>-0.169896</td>\n",
       "      <td>0.222888</td>\n",
       "      <td>-0.309302</td>\n",
       "      <td>...</td>\n",
       "      <td>2.64918</td>\n",
       "      <td>-1.479811</td>\n",
       "      <td>-0.328492</td>\n",
       "      <td>-0.296994</td>\n",
       "      <td>2.915737</td>\n",
       "      <td>-0.634027</td>\n",
       "      <td>-1.419315</td>\n",
       "      <td>0.998854</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Question_count  First_ratio  Second_ratio  Tag_count  pos_ratio  neg_ratio  \\\n",
       "0         0.40729      1.32493     -0.152399   0.665565    0.57509   0.289281   \n",
       "\n",
       "   subjectivity  polarity  senti_diffs_per_ref  Sticker_count  \\\n",
       "0      0.551111 -0.169896             0.222888      -0.309302   \n",
       "\n",
       "           ...            img text img img text  img text img text img  \\\n",
       "0          ...                          2.64918              -1.479811   \n",
       "\n",
       "   text img img img img  text img img img text  text img img text img  \\\n",
       "0             -0.328492              -0.296994               2.915737   \n",
       "\n",
       "   text img text img img  text img text img text      prob  predict_classes  \\\n",
       "0              -0.634027               -1.419315  0.998854                1   \n",
       "\n",
       "   predict_cluster_class  \n",
       "0                      7  \n",
       "\n",
       "[1 rows x 34 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['사실/NNG',\n",
       " '가장/MAG',\n",
       " '처음/NNG',\n",
       " '가장/MAG',\n",
       " '사람/NNG;이/JKS',\n",
       " '사실/NNG',\n",
       " '하나/NR',\n",
       " '예전/NNG',\n",
       " '사실/NNG',\n",
       " '주장/NNG;도/JX',\n",
       " '이전/NNG',\n",
       " '사실/NNG',\n",
       " '측면/NNG;이/JKS',\n",
       " '대폭/MAG',\n",
       " '장/NNG',\n",
       " '아마/MAG',\n",
       " '역사/NNG;의/JKG',\n",
       " '절반/NNG',\n",
       " '마음/NNG;이/JKS',\n",
       " '사실/NNG',\n",
       " '매니아/NNG',\n",
       " '달리/VV',\n",
       " '과거/NNG',\n",
       " '함께/MAG',\n",
       " '부분/NNG;이/JKS',\n",
       " '하나/NR;의/JKG',\n",
       " '추억/NNG']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['나돌/VV',\n",
       " '이번/NNG',\n",
       " '한/NNG',\n",
       " '벌써/MAG',\n",
       " '꾸준히/MAG',\n",
       " '벌써/MAG',\n",
       " '지/NNG',\n",
       " '것/NNB;을/JKO',\n",
       " '지/NNG',\n",
       " '많이/MAG',\n",
       " '논란/NNG;이/JKS',\n",
       " '것/NNB;도/JX',\n",
       " '충격/NNG',\n",
       " '글/NNG;을/JKO',\n",
       " '때/NNG',\n",
       " '안/NNG',\n",
       " '핵심/NNG',\n",
       " '때/NNG;도/JX',\n",
       " '줄/NNG',\n",
       " '빨리/MAG',\n",
       " '경우/NNG;가/JKS',\n",
       " '글/NNG']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
