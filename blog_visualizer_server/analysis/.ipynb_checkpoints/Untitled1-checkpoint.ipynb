{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import requests\n",
    "from selenium import webdriver\n",
    "from pykospacing import spacing\n",
    "import pandas as pd\n",
    "from konlpy.tag import Kkma\n",
    "import numpy as np\n",
    "import pickle\n",
    "from keras.models import load_model\n",
    "import jpype\n",
    "import tensorflow as tf\n",
    "from sklearn import linear_model\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "\n",
    "\n",
    "# prob = 1\n",
    "# predict_classes = 1\n",
    "kkma = Kkma()\n",
    "sentiment = pd.read_csv('static/polarity.csv')\n",
    "word_list = sentiment['ngram'].tolist()\n",
    "label = sentiment['max.value'].tolist()\n",
    "\n",
    "Category = np.array(['IT·컴퓨터', '건강·의학', '공연·전시', '교육·학문', '국내여행', '드라마·방송', '등산·낚시·레저',\n",
    "       '만화·애니', '맛집', '사진', '스포츠', '시사·인문·경제', '어학·외국어', '와인·술', '육아·결혼',\n",
    "       '자동차', '차·커피·디저트', '패션·뷰티'])\n",
    "\n",
    "Category_average_img_count = {'IT·컴퓨터': 10.128282828282828,\n",
    " '건강·의학': 8.582714382174206,\n",
    " '공연·전시': 12.55934065934066,\n",
    " '교육·학문': 10.158496732026144,\n",
    " '국내여행': 26.041441441441442,\n",
    " '드라마·방송': 11.486739469578783,\n",
    " '등산·낚시·레저': 23.269372693726936,\n",
    " '만화·애니': 17.366032210834554,\n",
    " '맛집': 18.8370720188902,\n",
    " '사진': 13.06946876824285,\n",
    " '스포츠': 8.87403314917127,\n",
    " '시사·인문·경제': 5.872727272727273,\n",
    " '어학·외국어': 11.586510263929618,\n",
    " '와인·술': 7.527027027027027,\n",
    " '육아·결혼': 15.317307692307692,\n",
    " '자동차': 17.298047276464544,\n",
    " '차·커피·디저트': 14.595382746051033,\n",
    " '패션·뷰티': 14.654545454545454}\n",
    "\n",
    "Category_average_text_len = {'IT·컴퓨터': 1245.679798090909,\n",
    " '건강·의학': 1300.688048757597,\n",
    " '공연·전시': 1468.787912648351,\n",
    " '교육·학문': 1380.7295753839874,\n",
    " '국내여행': 1626.1315322342334,\n",
    " '드라마·방송': 1351.5803434321374,\n",
    " '등산·낚시·레저': 1542.4686354612543,\n",
    " '만화·애니': 1016.5578340849175,\n",
    " '맛집': 1341.5159389492333,\n",
    " '사진': 764.9451266549917,\n",
    " '스포츠': 1743.537016895028,\n",
    " '시사·인문·경제': 1863.716667424242,\n",
    " '어학·외국어': 12064.428153108505,\n",
    " '와인·술': 1041.2500004054054,\n",
    " '육아·결혼': 1268.576924326924,\n",
    " '자동차': 2040.659815323742,\n",
    " '차·커피·디저트': 1292.0935606075336,\n",
    " '패션·뷰티': 1270.7898993686888}\n",
    "\n",
    "# Data preprocessing\n",
    "def Spacing_text(text_list):\n",
    "    spacing_list = []\n",
    "    for i in text_list:\n",
    "        if len(i) < 197:\n",
    "            spacing_list.append(spacing(i))\n",
    "        else:\n",
    "            iteration = int(len(i) / 197)\n",
    "            mod = len(i) % 197\n",
    "            start = 0\n",
    "            end = 197\n",
    "            check = 0\n",
    "            while True:\n",
    "                # 시행횟수 < 몫\n",
    "                if check < iteration:\n",
    "                    spacing_list.append(spacing(i[start:end]))\n",
    "                    start += 197\n",
    "                    end += 197\n",
    "                    check += 1\n",
    "                else:\n",
    "                    # 마지막 횟수 + 나머지 더 slice\n",
    "                    spacing_list.append(spacing(i[iteration * 197:(iteration * 197) + mod]))\n",
    "                    break\n",
    "    return spacing_list\n",
    "\n",
    "def remove_odd(x):\n",
    "    x = re.sub(\"nbsp\", \" \", x)\n",
    "    x = re.sub(\"\\xa0\", \"\", x)\n",
    "    x = re.sub(\"\\u200b\", \"\", x)\n",
    "    x = re.sub(\"\\n\", \"\", x)\n",
    "    x = re.sub(\"\\t\", \"\", x)\n",
    "    x = re.sub('   ', ' ', x)\n",
    "    return x\n",
    "\n",
    "def tfidf_vectorizer(Text):\n",
    "    v_load = pickle.load(open(\"static/[Structure_tag]TFIDF_features_100.pkl\", \"rb\"))\n",
    "    try:\n",
    "        return v_load.transform([Text]).toarray().flatten()\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "\n",
    "class textclass:\n",
    "    def Extract_structure_and_tag(User_id, Post_id):\n",
    "        url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "        r = requests.get(url)\n",
    "        bs = BeautifulSoup(re.sub('&nbsp;', ' ', r.text).encode(\"utf-8\"), \"html.parser\")\n",
    "        # structure\n",
    "        structure = bs.find(\"div\", {\"id\": \"postViewArea\"})\n",
    "        if structure == None:\n",
    "            structure = bs.find(\"div\", {\"class\", \"se_component_wrap sect_dsc __se_component_area\"})\n",
    "        # Title\n",
    "        title = bs.find(\"h3\", {\"class\": \"se_textarea\"})\n",
    "\n",
    "        # 스마트에디터3 타이틀 제거 임시 적용 (클래스가 다름)\n",
    "        if (title == None):\n",
    "            title = bs.find(\"span\", {\"class\": \"pcol1 itemSubjectBoldfont\"})\n",
    "        if (title != None):\n",
    "            title = title.text.strip()\n",
    "        else:\n",
    "            title = \"TITLE ERROR\"\n",
    "        \n",
    "        structure_p_img_tag = structure.find_all(['p', 'img'])\n",
    "        structure_dict = {'structure': structure, 'structure_p_img_tag': structure_p_img_tag,'Title':title}\n",
    "        # structure_p_img_tag : p,img tag만 extract\n",
    "        # structure : 모든 tag 가져오기\n",
    "        return structure_dict\n",
    "\n",
    "    # Extract_structure_and_tag 함수의 'structure_p_img_tag' 값을 가져와야함.\n",
    "\n",
    "    def HTML_preprocessing(structure_p_img_tag):\n",
    "        # only tag & text extract\n",
    "        tag_list = []\n",
    "        text_list = []\n",
    "        for i in structure_p_img_tag:\n",
    "            # p_tag만 불러오기\n",
    "            if \"<p\" in (str(i)):\n",
    "                tag_list.append('<p>')\n",
    "                # img만 있을 때\n",
    "\n",
    "                if '<img' in str(i):\n",
    "                    for j in i:\n",
    "                        try:\n",
    "                            if len(j.text) > 1:\n",
    "                                tag_list.append('<br>')\n",
    "                                text_list.append(j.text)\n",
    "                        except:\n",
    "                            pass\n",
    "\n",
    "                # img가 아닌 경우 span tag가 더 있을 때\n",
    "                elif '<span' in str(i):\n",
    "                    for j in i:\n",
    "                        if '<br' in str(j):\n",
    "                            text_list.append(j.text)\n",
    "                            # br_tag가 2개 이상 있을 때\n",
    "\n",
    "                            if len(j.findAll('br')) > 2:\n",
    "                                for _ in range(0, len(j.findAll('br'))):\n",
    "                                    tag_list.append('<br>')\n",
    "\n",
    "                            # br_tag가 1개 있을 때\n",
    "                            else:\n",
    "                                tag_list.append('<br>')\n",
    "\n",
    "                        # span은 있지만 br tag가 없을 때\n",
    "                        else:\n",
    "                            try:\n",
    "                                text_list.append(j.text)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "                # 그냥 p_tag만 있을 때 br_tag 추가\n",
    "                else:\n",
    "                    # 글이 있을 때\n",
    "                    if len(i.text) > 1:\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                    # 글 없이 br tag만 있을 때\n",
    "                    else:\n",
    "                        tag_list.append('<br>')\n",
    "                        text_list.append(i.text)\n",
    "\n",
    "                # P_tag 끝맽음\n",
    "                tag_list.append('</p>')\n",
    "\n",
    "            else:\n",
    "                tag_list.append('<img>')\n",
    "\n",
    "        text_list = list(map(remove_odd, text_list))\n",
    "        filter_text = list(filter(lambda x: len(x) > 1, text_list))\n",
    "\n",
    "        Text = \" \".join(list(filter(lambda x: len(x) > 1, map(lambda x: x.strip(), text_list))))\n",
    "        Text = re.sub('\\n', '', Text)\n",
    "        Text = re.sub('\\t', '', Text)\n",
    "        Space_text = \" \".join(Spacing_text(filter_text))\n",
    "        Count_space_mistake = len(Space_text) - len(Text)\n",
    "\n",
    "        # only tag\n",
    "        Structure_only_tag = \"|\".join(tag_list)\n",
    "        Structure_only_tag_df = pd.DataFrame({'text': [Structure_only_tag]})\n",
    "        array_temp = Structure_only_tag_df['text'].apply(\n",
    "            lambda x: \" img \".join(list(map(lambda x: 'text' if len(x) > 3 else '', x.split('<img>')))).strip().replace(\n",
    "                '  ', ' ')).values\n",
    "        refined_structure = ''.join(array_temp)\n",
    "\n",
    "        HTML_preprocessing = {'Text': Text, 'refined_structure': refined_structure,\n",
    "                              'Count_space_mistake': Count_space_mistake}\n",
    "\n",
    "        return HTML_preprocessing\n",
    "\n",
    "    def sentimental_analysis(text):\n",
    "        pos_word_list = []\n",
    "        neg_word_list = []\n",
    "        neut_word_list = []\n",
    "        pos_ratio = 0.000000001\n",
    "        neg_ratio = 0.000000001\n",
    "        subjectivity = 0.000000001\n",
    "        polarity = 0.000000001\n",
    "        senti_diffs_per_ref = 0.000000001\n",
    "\n",
    "        if text == '':\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list\n",
    "        else:\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            neut = 0\n",
    "\n",
    "            text = text.split(' ')\n",
    "            n = len(text)\n",
    "            for i in text:\n",
    "                i = remove_odd(i)\n",
    "                jpype.attachThreadToJVM()\n",
    "                pre = kkma.pos(i)\n",
    "                test = ';'.join(['/'.join(i) for i in pre])\n",
    "                if test in word_list:\n",
    "                    if label[word_list.index(test)] == 'POS':\n",
    "                        pos += 1\n",
    "                        pos_word_list.append(test)\n",
    "                    elif label[word_list.index(test)] == 'NEG':\n",
    "                        neg += 1\n",
    "                        neg_word_list.append(test)\n",
    "                    elif label[word_list.index(test)] == 'NEUT':\n",
    "                        neut +=1\n",
    "                        neut_word_list.append(test)\n",
    "            try:\n",
    "                pos_ratio = pos / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                neg_ratio = neg / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                subjectivity = (neg + pos) / n\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                polarity = (neg - pos) / (neg + pos)\n",
    "            except:\n",
    "                pass\n",
    "            try:\n",
    "                senti_diffs_per_ref = (pos - neg) / n\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            sentiment_dict = {'pos_ratio': pos_ratio, 'neg_ratio': neg_ratio, 'subjectivity': subjectivity,\n",
    "                              'polarity': polarity, 'senti_diffs_per_ref': senti_diffs_per_ref}\n",
    "            return sentiment_dict, pos_word_list, neg_word_list, neut_word_list\n",
    "\n",
    "    def check_First_second(Text):\n",
    "        first_person = ['나/NP', '저/NP', '내/NP', '제/NP', '저희/NP', '우리/NP']\n",
    "        second_person = ['너/NP', '자네/NP', '당신/NP', '그대/NP', '그쪽/NP', '너희/NP', '자기/NP']\n",
    "        First = 0\n",
    "        Second = 0\n",
    "        if Text == '':\n",
    "            check_First_second_dict = {'First': First, 'Second': Second}\n",
    "            return check_First_second_dict\n",
    "        else:\n",
    "            text = kkma.pos(Text)\n",
    "            for i in text:\n",
    "                temp = \"/\".join(i)\n",
    "                if temp in first_person:\n",
    "                    First += len(temp.split('/')[0])\n",
    "                if temp in second_person:\n",
    "                    Second += len(temp.split('/')[0])\n",
    "            check_First_second_dict = {'First_ratio': First / len(Text), 'Second_ratio': Second / len(Text)}\n",
    "            return check_First_second_dict\n",
    "\n",
    "class otherclass:\n",
    "    def effort_check(Category,Text_len,Img_count):\n",
    "        effort_text = Text_len / Category_average_text_len[Category]\n",
    "        effort_img = Img_count / Category_average_img_count[Category]\n",
    "\n",
    "        effort_dict = {'effort_ratio':effort_text,'effort_img_ratio':effort_img}\n",
    "        return effort_dict\n",
    "\n",
    "    def Tag_count(url):        \n",
    "#         driver = webdriver.Chrome('static/chromedriver')\n",
    "#         driver.get(url)\n",
    "#         driver.implicitly_wait(10)\n",
    "#         tag_count = len(re.sub('\\n','',driver.find_element_by_class_name('wrap_tag').text).strip().split('#')[1:])\n",
    "        caps = DesiredCapabilities().CHROME\n",
    "        caps[\"pageLoadStrategy\"] = \"normal\"  #  complete\n",
    "        driver = webdriver.PhantomJS('static/phantomjs-2.1.1-macosx/bin/phantomjs',service_args=options,desired_capabilities=caps)\n",
    "        # URL 읽어 들이기\n",
    "        driver.get(url)\n",
    "        tag_count = len(driver.find_element_by_class_name('wrap_tag').text.split('#')[1:])\n",
    "        tag_dict = {'tag_count':tag_count}\n",
    "        driver.close()\n",
    "        return tag_dict\n",
    "\n",
    "class htmlclass:\n",
    "    def Extract_Alignment(structure):\n",
    "        # Align\n",
    "        split_structure =  remove_odd(str(structure)).split('>')\n",
    "        center= 0\n",
    "        left = 0\n",
    "        right = 0\n",
    "        justify = 0\n",
    "        for item in split_structure:\n",
    "            if 'align' or 'ALIGN' in str(item):\n",
    "                if 'center' in str(item):\n",
    "                    center +=1\n",
    "                if 'left' in str(item):\n",
    "                    left += 1\n",
    "                if 'right' in str(item):\n",
    "                    right +=1\n",
    "                if 'justify' in str(item):\n",
    "                    justify +=1\n",
    "        Align = [left, center, right, justify]\n",
    "        return Align\n",
    "\n",
    "    def Extract_Sticker_count(structure):\n",
    "        Sticker_count = 0\n",
    "        # Sticker_count로 수정\n",
    "        sticker_img = structure.find_all('a')\n",
    "        for i in sticker_img:\n",
    "            if 'sticker' in str(i):\n",
    "                Sticker_count += 1\n",
    "        return Sticker_count\n",
    "\n",
    "\n",
    "# class chorme_class:\n",
    "#     def user_information(mobile_url, opening_url):\n",
    "#         driver = webdriver.Chrome('./static/chromedriver')\n",
    "#\n",
    "#         driver.get(mobile_url)\n",
    "#         driver.implicitly_wait(3)\n",
    "#\n",
    "#         # 존재하지 않는 게시물 클릭\n",
    "#         driver.find_element_by_class_name(\"btn_area\").click()\n",
    "#         driver.implicitly_wait(3)\n",
    "#\n",
    "#         # Blog_name, Blog_nickname, Blog_mobile_profile_img, Blog_info_text\n",
    "#         Blog_name = driver.find_element_by_css_selector(\n",
    "#             '#rego_cover > div.cover_cont > div.tit_area > h2 > a > span').text\n",
    "#         Blog_nickname = driver.find_element_by_class_name(\"user_name\").text\n",
    "#\n",
    "#         # Count_neighbors\n",
    "#         neighbors_string = re.sub(\",\", \"\", driver.find_element_by_class_name(\"count_buddy\").text)\n",
    "#         Count_neighbors = int(re.findall('\\d+', neighbors_string)[0])\n",
    "#\n",
    "#         # Count_visitors\n",
    "#         visitor_stirng = driver.find_elements_by_class_name('count')[0].text\n",
    "#         Count_visitors = re.sub(\",\", \"\", visitor_stirng.split(\"전체\")[1]).strip()\n",
    "#         # Opening URL\n",
    "#         driver.get(opening_url)\n",
    "#         driver.implicitly_wait(3)\n",
    "#         driver.find_element_by_id('category2').click()\n",
    "#         years = driver.find_elements_by_css_selector(\n",
    "#             '#post-area > div:nth-child(4) > table:nth-child(2) > tbody > tr > td > table > tbody > tr > td > table > tbody > tr:nth-child(3) > td:nth-child(2)')\n",
    "#         blog_opening_date = \"NO_DATA\"\n",
    "#         for year in years:\n",
    "#             if len(year.text) > 4:\n",
    "#                 blog_opening_date = year.text\n",
    "#         driver.close()\n",
    "#\n",
    "#         user_information_dict = {\n",
    "#             'Blog_name': Blog_name,\n",
    "#             'Blog_nickname': Blog_nickname,\n",
    "#             'Count_neighbors': Count_neighbors,\n",
    "#             'Count_visitors': Count_visitors,\n",
    "#             'blog_opening_date': blog_opening_date}\n",
    "#\n",
    "#         return user_information_dict\n",
    "\n",
    "# Test\n",
    "User_id = 'newpark314'\n",
    "Post_id = '221387605004'\n",
    "Category = '맛집'\n",
    "\n",
    "def get_naver_post_all_data():\n",
    "    Category = np.array(['IT·컴퓨터', '건강·의학', '공연·전시', '교육·학문', '국내여행', '드라마·방송', '등산·낚시·레저',\n",
    "       '만화·애니', '맛집', '사진', '스포츠', '시사·인문·경제', '어학·외국어', '와인·술', '육아·결혼',\n",
    "       '자동차', '차·커피·디저트', '패션·뷰티'])\n",
    "# start\n",
    "    url = \"http://blog.naver.com/PostView.nhn?blogId=\" + User_id + \"&logNo=\" + Post_id + \"&redirect=Dlog&widgetTypeCall=true\"\n",
    "    mobile_url = \"http://m.blog.naver.com/PostView.nhn?blogId=\"+ User_id\n",
    "    opening_url = 'http://blog.naver.com/profile/intro.nhn?blogId='+ User_id\n",
    "\n",
    "# Text_len,Question_count,Sentiment(pos_ratio,neg_ratio,subjectivity,polarity,sentiment_diff_ref)\n",
    "# Count_Space_mistake, First_ratio, Second_ratio\n",
    "\n",
    "    structure = textclass.Extract_structure_and_tag(User_id,Post_id)\n",
    "    all_tag = structure['structure']\n",
    "    p_img_tag = structure['structure_p_img_tag']\n",
    "    title = structure['Title']\n",
    "    HTML_preprocessing = textclass.HTML_preprocessing(p_img_tag)\n",
    "    text = HTML_preprocessing['Text']\n",
    "\n",
    "# Category prediction\n",
    "    #Tfidf_model로 교체\n",
    "    tfidf_text = 'tfidf_text.pkl'\n",
    "    tfidf_title = 'tfidf_title.pkl'\n",
    "    logreg_model = 'Category_model.pkl'\n",
    "    logreg = pickle.load(open(logreg_model, 'rb'))\n",
    "    vectorizer_Text = pickle.load(open(tfidf_text, 'rb'))\n",
    "    vectorizer_Title = pickle.load(open(tfidf_title, 'rb'))\n",
    "    text_vec = vectorizer_Text.transform([text])\n",
    "    title_vec = vectorizer_Title.transform([title])\n",
    "    text_vec = pd.DataFrame(text_vec.toarray())\n",
    "    title_vec = pd.DataFrame(title_vec.toarray())\n",
    "    x = pd.concat([text_vec,title_vec],axis=1)\n",
    "    Category = Category[logreg.predict(x)[0]]\n",
    "\n",
    "# variables\n",
    "    Text_len = len(text)\n",
    "    Count_Space_mistake = HTML_preprocessing['Count_space_mistake']\n",
    "    Question_count = text.count('?')\n",
    "    sentiment_pre = textclass.sentimental_analysis(text)\n",
    "    sentiment = sentiment_pre[0]\n",
    "    pos_word = sentiment_pre[1]\n",
    "    neg_word = sentiment_pre[2]\n",
    "    neut_word = sentiment_pre[3]\n",
    "    first_second = textclass.check_First_second(text)\n",
    "\n",
    "# Alignment(Left,Center,Right,Justify), Sticker_count\n",
    "# variables\n",
    "    Alignment = htmlclass.Extract_Alignment(all_tag)\n",
    "    Sticker_count = htmlclass.Extract_Sticker_count(all_tag)\n",
    "\n",
    "# Effort(effort_ratio,effort_img_ratio), Tag_count\n",
    "    refined_structure = HTML_preprocessing['refined_structure']\n",
    "\n",
    "# variables\n",
    "    Img_count = refined_structure.count('img')\n",
    "    Effort = otherclass.effort_check(Category,Text_len,Img_count)\n",
    "    Tag_count = otherclass.Tag_count(url)['tag_count']\n",
    "\n",
    "# user_information\n",
    "# 사용자 정보\n",
    "#\tuser_information = chorme_class.user_information(mobile_url,opening_url)\n",
    "\n",
    "## user variables\n",
    "#\tBlog_name = user_information['Blog_name']\n",
    "#\tBlog_nickname = user_information['Blog_nickname']\n",
    "#\tCount_neighbors = user_information['Count_neighbors']\n",
    "#\tCount_visitors = user_information['Count_visitors']\n",
    "#\tblog_opening_date = user_information['blog_opening_date']\n",
    "\n",
    "# tfidf\n",
    "\n",
    "# tfidf, keras model, cluster model\n",
    "\n",
    "# variable\n",
    "    Structure_13 = tfidf_vectorizer(refined_structure)\n",
    "\n",
    "# Data organization\n",
    "\n",
    "    list_1 = [Question_count,first_second['First_ratio'],first_second['Second_ratio'],Tag_count,\n",
    "    sentiment['pos_ratio'],sentiment['neg_ratio'],sentiment['subjectivity'],sentiment['polarity'],sentiment['senti_diffs_per_ref'],\n",
    "    Sticker_count,Text_len,Count_Space_mistake,Effort['effort_ratio'],Effort['effort_img_ratio'],\n",
    "    Alignment[0],Alignment[1],Alignment[2],Alignment[3]]\n",
    "    list_2 = Structure_13.tolist()\n",
    "\n",
    "    total_dataset = list_1 + list_2\n",
    "    origin_df = pd.DataFrame(total_dataset).T\n",
    "    origin_df.columns = ['Question_count','First_ratio','Second_ratio','Tag_count','pos_ratio',\n",
    "     'neg_ratio','subjectivity','polarity','senti_diffs_per_ref','Sticker_count','Text_len','Count_space_mistake',\n",
    "       'effort_ratio','effort_img_ratio','Left','Center','Right','Justify',\n",
    "       'img img img img img','img img img img text','img img img text img','img img text img img','img img text img text',\n",
    "     'img text img img img','img text img img text','img text img text img','text img img img img','text img img img text',\n",
    "     'text img img text img','text img text img img','text img text img text']\n",
    "\n",
    "# keras\n",
    "## Standard scaler model load\n",
    "\n",
    "    scalerfile = 'static/scaler.pkl'\n",
    "    scaler = pickle.load(open(scalerfile, 'rb'))\n",
    "# transform data\n",
    "    origin_df = origin_df.fillna(0)\n",
    "    X_scaled = pd.DataFrame(scaler.transform(origin_df),columns = origin_df.columns)\n",
    "\n",
    "# MLP model load\n",
    "    mlp_clf = 'MLP.pkl'\n",
    "    predict_classes = mlp_clf.predict(test.values.reshape((1, 31))).item()\n",
    "    prob = mlp_clf.predict_proba(test.values.reshape((1, 31)))[0][1]\n",
    "       \n",
    "# keras model load\n",
    "\n",
    "#    graph = tf.get_default_graph()\n",
    "#    with graph.as_default():\n",
    "#        # data type reshape & predict probability\n",
    "#        prob = model.predict(X_scaled.values.reshape((1, 31))).item()\n",
    "#        predict_classes = model.predict_classes(X_scaled.values.reshape((1, 31))).item()\n",
    "#\n",
    "#    test = load_model(X_scaled)\n",
    "#    prob = test['prob']\n",
    "#    predict_classes = test['predict_classes']\n",
    "\n",
    "# Cluster는 cluster model자체가 scaler로 된 모델이라 그냥 origin 값 집어 넣어야함.\n",
    "    clusterfile = '8-means(0,1,2,5,6,7).pkl'\n",
    "    cluster = pickle.load(open(clusterfile, 'rb'))\n",
    "# predict cluster class\n",
    "    predict_cluster_class = cluster.predict(origin_df).item()\n",
    "\n",
    "# Final save csv file\n",
    "    Predict_df = pd.DataFrame({'prob':prob,'predict_classes':predict_classes,'predict_cluster_class':predict_cluster_class},index=[0])\n",
    "#user_df = pd.DataFrame({'Blog_name':Blog_name,'Blog_nickname':Blog_nickname,'Count_neighbors':Count_neighbors,'Count_visitors':Count_visitors,'blog_opening_date':blog_opening_date},index=[0])\n",
    "    value = pd.concat([X_scaled,Predict_df],axis=1)\n",
    "#pos_word\n",
    "#\tneg_word\n",
    "\n",
    "    Predict_dict = {'prob':prob,'predict_classes':predict_classes,'predict_cluster_class':predict_cluster_class}\n",
    "    #user_dict = {'Blog_name':Blog_name,'Blog_nickname':Blog_nickname,'Count_neighbors':Count_neighbors,'Count_visitors':Count_visitors,'blog_opening_date':blog_opening_date}\n",
    "    #\n",
    "\n",
    "    refined_X_scaled = X_scaled.T.to_dict()[0]\n",
    "\n",
    "    Structure = {\n",
    "        'img img img img img': refined_X_scaled['img img img img img'],\n",
    "        'img img img img text':refined_X_scaled['img img img img text'],\n",
    "        'img img img text img':refined_X_scaled['img img img text img'],\n",
    "        'img img text img img' :refined_X_scaled['img img text img img'],\n",
    "        'img img text img text':refined_X_scaled['img img text img text'],\n",
    "        'img text img img img':refined_X_scaled['img text img img img'],\n",
    "        'img text img img text':refined_X_scaled['img text img img text'],\n",
    "        'img text img text img':refined_X_scaled['img text img text img'],\n",
    "        'text img img img img':refined_X_scaled['text img img img img'],\n",
    "        'text img img img text':refined_X_scaled['text img img img text'],\n",
    "        'text img img text img':refined_X_scaled['text img img text img'],\n",
    "        'text img text img img':refined_X_scaled['text img text img img'],\n",
    "        'text img text img text':refined_X_scaled['text img text img text']}\n",
    "\n",
    "    Sentiment = {\n",
    "        'pos_ratio': refined_X_scaled['pos_ratio'],\n",
    "        'neg_ratio':refined_X_scaled['neg_ratio'],\n",
    "        'subjectivity':refined_X_scaled['subjectivity'],\n",
    "        'polarity' :refined_X_scaled['polarity'],\n",
    "        'senti_diffs_per_ref':refined_X_scaled['senti_diffs_per_ref']\n",
    "    }\n",
    "\n",
    "    Other = {\n",
    "        'Question_count':refined_X_scaled['Question_count'],\n",
    "        'First_ratio': refined_X_scaled['First_ratio'],\n",
    "        'Second_ratio':refined_X_scaled['Second_ratio'],\n",
    "        'Tag_count':refined_X_scaled['Tag_count'],\n",
    "        'Sticker_count':refined_X_scaled['Sticker_count'],\n",
    "        'Text_len':refined_X_scaled['Text_len'],\n",
    "        'Count_space_mistake':refined_X_scaled['Count_space_mistake'],\n",
    "        'effort_ratio':refined_X_scaled['effort_ratio'],\n",
    "        'effort_img_ratio':refined_X_scaled['effort_img_ratio'],\n",
    "        'Left' :refined_X_scaled['Left'],\n",
    "        'Center':refined_X_scaled['Center'],\n",
    "        'Right':refined_X_scaled['Right'],\n",
    "        'Justify':refined_X_scaled['Justify']}\n",
    "\n",
    "    all_data = {\n",
    "        'Structure': Structure,\n",
    "        'Sentiment': Sentiment,\n",
    "        'Other': Other,\n",
    "        'Predict': Predict_dict,\n",
    "        #'User': user_dict,\n",
    "        'words': {\n",
    "            'positive': pos_word,\n",
    "            'negative': neg_word,\n",
    "            'neutral' : neut_word\n",
    "        }\n",
    "    }\n",
    "\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "User_id = 'newpark314'\n",
    "Post_id = '221387605004'\n",
    "Category = '맛집'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'Category' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-2ac86ed8d05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_naver_post_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-7b64ad2390ab>\u001b[0m in \u001b[0;36mget_naver_post_all_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0mtitle_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtitle_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext_vec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtitle_vec\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mCategory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCategory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlogreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;31m# variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'Category' referenced before assignment"
     ]
    }
   ],
   "source": [
    "get_naver_post_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) \\ufeff안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 \\'애드포스트\\'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 \\'오래 됨\\'의 기준이 무엇이냐? 바로 \\'글을 쓴 지 1년이 넘어 가는 글\\'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 \\'버스터미널\\'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 \\'니가 검색비허용을 시간표 논란이 없어지지 않느냐\\'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 \\'이 포스트의 시간표는 맞지 않을 수 있습니다.\\'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 \\'니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야\\'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 \\'1만 건/1일\\'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 \\'버스 터미널 시간표\\'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 \\'해피빈\\' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2ac86ed8d05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_naver_post_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-9661d28ed6cf>\u001b[0m in \u001b[0;36mget_naver_post_all_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mvectorizer_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0mvectorizer_Title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0mtext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_Text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0mtitle_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_Title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mtext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[0;32m--> 681\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) \\ufeff안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 \\'애드포스트\\'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 \\'오래 됨\\'의 기준이 무엇이냐? 바로 \\'글을 쓴 지 1년이 넘어 가는 글\\'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 \\'버스터미널\\'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 \\'니가 검색비허용을 시간표 논란이 없어지지 않느냐\\'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 \\'이 포스트의 시간표는 맞지 않을 수 있습니다.\\'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 \\'니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야\\'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 \\'1만 건/1일\\'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 \\'버스 터미널 시간표\\'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 \\'해피빈\\' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'"
     ]
    }
   ],
   "source": [
    "get_naver_post_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-2bf3a0e5b7af>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-2bf3a0e5b7af>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    text = 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) ﻿안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 '애드포스트'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 '오래 됨'의 기준이 무엇이냐? 바로 '글을 쓴 지 1년이 넘어 가는 글'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 '버스터미널'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 '니가 검색비허용을 시간표 논란이 없어지지 않느냐'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 '이 포스트의 시간표는 맞지 않을 수 있습니다.'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 '니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 '1만 건/1일'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 '버스 터미널 시간표'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 '해피빈' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'\u001b[0m\n\u001b[0m                                                                                                                                                                                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "text = 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) ﻿안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 '애드포스트'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 '오래 됨'의 기준이 무엇이냐? 바로 '글을 쓴 지 1년이 넘어 가는 글'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 '버스터미널'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 '니가 검색비허용을 시간표 논란이 없어지지 않느냐'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 '이 포스트의 시간표는 맞지 않을 수 있습니다.'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 '니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 '1만 건/1일'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 '버스 터미널 시간표'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 '해피빈' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text = 'tfidf_text.pkl'\n",
    "tfidf_title = 'tfidf_title.pkl'\n",
    "logreg_model = 'Category_model.pkl'\n",
    "logreg = pickle.load(open(logreg_model, 'rb'))\n",
    "vectorizer_Text = pickle.load(open(tfidf_text, 'rb'))\n",
    "vectorizer_Title = pickle.load(open(tfidf_title, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec = vectorizer_Text.transform([text])\n",
    "title_vec = vectorizer_Title.transform([title])\n",
    "text_vec = pd.DataFrame(text_vec.toarray())\n",
    "title_vec = pd.DataFrame(title_vec.toarray())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x2000 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 139 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_Text.transform([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) ﻿안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 '애드포스트'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 '오래 됨'의 기준이 무엇이냐? 바로 '글을 쓴 지 1년이 넘어 가는 글'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 '버스터미널'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 '니가 검색비허용을 시간표 논란이 없어지지 않느냐'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 '이 포스트의 시간표는 맞지 않을 수 있습니다.'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 '니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 '1만 건/1일'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 '버스 터미널 시간표'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 '해피빈' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) \\ufeff안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 \\'애드포스트\\'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 \\'오래 됨\\'의 기준이 무엇이냐? 바로 \\'글을 쓴 지 1년이 넘어 가는 글\\'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 \\'버스터미널\\'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 \\'니가 검색비허용을 시간표 논란이 없어지지 않느냐\\'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 \\'이 포스트의 시간표는 맞지 않을 수 있습니다.\\'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 \\'니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야\\'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 \\'1만 건/1일\\'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 \\'버스 터미널 시간표\\'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 \\'해피빈\\' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-2ac86ed8d05c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_naver_post_all_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-41-5367ff478e33>\u001b[0m in \u001b[0;36mget_naver_post_all_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    417\u001b[0m     \u001b[0mvectorizer_Text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m     \u001b[0mvectorizer_Title\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf_title\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m     \u001b[0mtext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_Text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0mtitle_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_Title\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[0mtext_vec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py/lib/python3.6/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    679\u001b[0m         \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[0;32m--> 681\u001b[0;31m                         estimator=self, dtype=FLOAT_DTYPES)\n\u001b[0m\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/py/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    431\u001b[0m                                       force_all_finite)\n\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 433\u001b[0;31m         \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'From. 블로그씨 나도 답하기▶ 블로그와 함께한 나만의 추억은 무엇인가요?\"#15살블로그축하해\" 달고 블로그 이벤트에 참여해보세요.(자세히보기) \\ufeff안녕하십니까? 이번 포스트는 블로그와 함꼐하는 추억을 간단하게 언급해 보고자 합니다. 사실 저는 가장 기억에 남는 것이 블로그를 하면서 \\'애드포스트\\'라는 광고를 처음 달았을 때, 처음으로 수익금을 지급받았을 때가 가장 기억에 남습니다. 처음에는 한 달에 5000원도 못 벌어서 4~5달이 지나서야 1만원도 되지 않은 수익금을 받은 지가 엊그제같은데 벌써 애드포스트로 월 1만원 이상은 꾸준히 버는(?) 사람이 되었습니다. 사실 애드포스트는 블로그와는 별개의 서비스로 벌써 서비스를 시작한 지 10년이 다 되어 가는 것으로 알고 있습니다. 그럼 본격적으로 블로그와 함께했던 추억을 언급해 드리기 전에 하나 이야기해드리자면 저는 오래 된 글과 관련하여 댓글이나 쪽지를 받는 것을 좋아하지 않습니다. 그 \\'오래 됨\\'의 기준이 무엇이냐? 바로 \\'글을 쓴 지 1년이 넘어 가는 글\\'입니다. 저는 멘탈이라던가 악플 같은 부분을 받기 싫어 댓글란을 비허용하는 편입니다만 예전 글에는 허용된 포스트들이 많이 있습니다. 사실 저의 블로그는 \\'버스터미널\\'시간표가 주 컨텐츠인데 세월이 가면 버스 시간표는 바뀌기 마련이거든요. 물론 \\'니가 검색비허용을 시간표 논란이 없어지지 않느냐\\'라는 주장도 있는데 저도 그 주장에 동의해서 앞으로는 2017년 이전 포스트들은 전면 비공개하는 것도 검토하고 있습니다. 사실 저는 저의 포스트에 \\'이 포스트의 시간표는 맞지 않을 수 있습니다.\\'라는 멘트를 요즘에 쓰고 있는데 3~4년 전쯤에 어떤 분이 \\'니 블로그 시간표때문에 내가 안동을 못갔다 이새끼야\\'라는 취지의 댓글을 다셔서 (아마 저 말고 다른 매니아들도 경험하셨을 겁니다.)이런 충격 때문에 댓글을 허용하지 않는 측면이 있고요. 다른 분들 보고 시간표 좀 찍어서 저를 이기고 검색 상위에 올라 가라는 취지로 포스트 상단에 시간표가 바뀜을 알리는 내용을 적고 있습니다. 당장 영주나 안동 같은 경우도 2018년 11월부터 터미널 시간표가 대폭 바뀝니다. 그 점을 알고 여러 사람들이 최신 시간표를 블로그에 올려 놓은 상황이고요. 그럼 이쯤에서 본론으로 들어가도록 하겠습니다. 이미지 몇 장 보시고 하단에 멘트를 입력하면서 내용을 진행하겠습니다. *저의 블로그 근원이라고 할 수 있는 2010년 12월 포스트. 아마 경전선 진영역에 관한 내용일 겁니다. *물론 2008년에도 글을 쓴 적이 있지만 1년 넘게 접속을 하지 않아 리셋되었고 결과적으로는 2010년 12월이 저의 블로그 첫 시작인데 네이버 블로그 역사의 절반 이상을 함께하고 있음을 알 수 있습니다. 앞으로 5년, 10년, 나아가 제가 죽을 때 까지(얼마 안 남았을 수도 있음.) 네이버 블로그와 함께하면 좋겠습니다. *지금 작성한 글의 갯수만 2000건이 넘고 조회수가 \\'1만 건/1일\\'이 넘는 큰 블로그가 되었는데 자부심보다는 신뢰도가 추락할 까봐 두려운 마음이 더 큽니다. 오른쪽 이미지는 저의 블로그의 핵심 컨텐츠 중 하나인 야구 관련 내용, 특히 야구장이라던가 동대구복합환승센터 신축현장을 촬영할 때도 블로그를 통해 타 지역 사람들께 근황을 알려 줄 수 있어서 좋았습니다. *다만 저는 대구 지역 거주자는 아닙니다. *요즘에는 마산야구장+부전마산복선전철 조합으로 찍고 있습니다. 사실 저의 블로그 조회수의 대부분을 차지하는 것이 \\'버스 터미널 시간표\\'에서 나오는 데 저는 다른 버스 매니아 블로거들과 달리 버스 시간표가 바뀌면 A/S를 해 드리려고 노력하고 있습니다. 홈-페이지가 없는 터미널은 현지 분들이 저에게 노선 변경사항 쪽지를 주시면 제가 최대한 빨리 수정 반영하고 있으니 많은 양해를 바랍니다. 또한 요즘에는 울산역이라던가 몇몇 기차역들도 일간 조회수 TOP100에 드는 경우가 있습니다. *여행 후기 같은 경우 2009년에 했던 것까지 기록을 했는데 당시에는 디카 사진을 썼습니다. 향후 과거 여행 글은 비공개 전환 검토 중에 있습니다. 이상으로 블로그와 함께 했던 저의 추억을 포스팅해 봤는데요. 저에게 있어서 상당히 긍정적인 부분이 컸던 것 같습니다. 물론 글 포스팅을 통한 \\'해피빈\\' 기부도 하나의 추억으로 볼 수 있겠지요...... 이쯤에서 블로그씨 답변시리즈 208편 포스트 마치겠습니다. 이웃 블로거, 다른 모든 블로거 여러분들도 네이버 블로그를 통해 좋은 추억 만드시길 바랍니다. *네이버 블로그의 15주년을 축하드립니다.'"
     ]
    }
   ],
   "source": [
    "get_naver_post_all_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
